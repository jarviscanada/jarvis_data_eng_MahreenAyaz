{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c79fe-654e-4260-8dcb-3c71b9407757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will \n",
    "- learn the concept of ETL\n",
    "- write ETL jobs for CSV files from `pgexercises` https://pgexercises.com/gettingstarted.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5df111-08b1-4884-9c39-8e2ca4e71195",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# What's ETL or ELT?\n",
    "\n",
    "ETL stands for Extract, Transform, Load. In the context of Spark, ETL refers to the process of extracting data from various sources, transforming it into a desired format or structure, and loading it into a target system, such as a data warehouse or a data lake.\n",
    "\n",
    "Here's a breakdown of each step in the ETL process:\n",
    "\n",
    "## Extract\n",
    "This step involves extracting data from multiple sources, such as databases, files (CSV, JSON, Parquet), APIs, or streaming data sources. Spark provides connectors and APIs to read data from a wide range of sources, allowing you to extract data in parallel and efficiently handle large datasets.\n",
    "\n",
    "## Transform\n",
    "In the transform step, the extracted data is processed and transformed according to specific business logic or requirements. This may involve cleaning the data, applying calculations or aggregations, performing data enrichment, filtering, joining datasets, or any other data manipulation operations. Spark provides a powerful set of transformation functions and SQL capabilities to perform these operations efficiently in a distributed and scalable manner.\n",
    "\n",
    "## Load\n",
    "Once the data has been transformed, it is loaded into a target system, such as a data warehouse, a data lake, or another storage system. Spark allows you to write the transformed data to various output formats and storage systems, including databases, distributed file systems (like Hadoop Distributed File System or Amazon S3), or columnar formats like Delta Lake or Apache Parquet. The data can be partitioned, sorted, or structured to optimize querying and analysis.\n",
    "\n",
    "Spark's distributed computing capabilities, scalability, and rich ecosystem of libraries make it a popular choice for ETL workflows. It can handle large-scale data processing, perform complex transformations, and efficiently load data into different target systems.\n",
    "\n",
    "By leveraging Spark for ETL, organizations can extract data from diverse sources, apply transformations to ensure data quality and consistency, and load the transformed data into a central repository for further analysis, reporting, or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407fe3c4-590a-403f-9199-c2d221c20ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Enable DBFS UI\n",
    "\n",
    "- Setting -> Admin Console -> search for dbfs\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/entable_dbfs.jpg\" width=\"700\">\n",
    "\n",
    "- Refresh the page and view DBFS files from UI\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/dbfs%20ui.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ff3413-eea1-4cfb-a0c1-c4ed0e485121",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import `pgexercises` CSV files\n",
    "\n",
    "- The pgexercises CSV data files can be found [here](https://github.com/jarviscanada/jarvis_data_eng_demo/tree/feature/data/spark/data/pgexercises).\n",
    "- The pgexercises schema can be found [here](https://pgexercises.com/gettingstarted.html) (for reference purposes).\n",
    "- Upload the `bookings.csv`, `facilities.csv`, and `members.csv` files using Databricks UI (see screenshot)\n",
    "- You can view the imported files from the DBFS UI.\n",
    "\n",
    "![Upload Files](https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/spark/notebook/spark_fundamentals/img/upload%20file.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e85a8b2-6cf3-40af-92d6-74220a9ce9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Interview Questions\n",
    "\n",
    "While completing the rest of the practice, try to answer the following questions:\n",
    "\n",
    "## Concepts\n",
    "- What is ETL? (Hint: Explain each step)\n",
    "ETL stands for Extract, Transform, Load. It refers to the process of extracting data from various sources, transforming it into a format that is suitable for analysis or other downstream purposes, and then loading it into a target data storage system, typically a data warehouse or a database.\n",
    "## Databricks\n",
    "- What is Databricks?\n",
    "Databricks is a unified analytics platform designed to accelerate innovation by unifying data engineering, data science, and business analytics. It was founded by the creators of Apache Spark, an open-source distributed computing system for big data processing.\n",
    "\n",
    "- What is a Notebook?\n",
    "A notebook is an interactive computing environment that allows users to create and share documents containing live code, equations, visualizations, and narrative text. Notebooks are commonly used in data science, machine learning, and scientific computing for exploratory analysis, prototyping, and sharing research findings.\n",
    "\n",
    "- What is DBFS?\n",
    "DBFS stands for Databricks File System. It is a distributed file system that is part of the Databricks Unified Analytics Platform. DBFS provides a scalable and reliable storage solution for managing and accessing data within Databricks environments.\n",
    "\n",
    "- What is a cluster? \n",
    "A cluster refers to a group of interconnected computers (nodes) that work together to process and analyze large volumes of data. Each node in the cluster typically runs a distributed computing framework, such as Apache Spark, which allows for parallel processing of data across multiple machines.\n",
    "\n",
    "- Is Databricks a data lake or a data warehouse?\n",
    "\n",
    "Databricks is not strictly a data lake or a data warehouse; rather, it is a unified analytics platform that supports both data lake and data warehouse functionalities, among other features.\n",
    "\n",
    "## Managed Table\n",
    "- What is a managed table in Databricks?\n",
    "A managed table refers to a type of table where Databricks manages both the metadata and the data files associated with the table. When you create a managed table, Databricks takes care of storing the table's schema and location information in its internal metastore, and it also manages the actual data files stored in a designated directory within a file system, such as DBFS (Databricks File System) or a cloud storage service like Amazon S3 or Azure Data Lake Storage (ADLS).\n",
    "\n",
    "- Can you explain how to create a managed table in Databricks?\n",
    "-- Create a database (optional)\n",
    "CREATE DATABASE IF NOT EXISTS my_database;\n",
    "\n",
    "-- Use the database\n",
    "USE my_database;\n",
    "\n",
    "-- Create a managed table\n",
    "CREATE TABLE IF NOT EXISTS my_table (\n",
    "    column1_name datatype1,\n",
    "    column2_name datatype2,\n",
    "    ...\n",
    ") USING delta;\n",
    "\n",
    "- Can you compare a managed table with an RDBMS table? (Hint: Schema on read vs schema on write)\n",
    "In an RDBMS, such as MySQL or PostgreSQL, the schema of a table is defined when the data is written to the database.\n",
    "Before data can be inserted into the table, the table's schema (column names, data types, constraints) must be predefined and enforced by the database engine. Once data is inserted into the table, it must adhere to the predefined schema. Any deviations will result in errors during insertion.\n",
    "\n",
    "- What is the Hive metastore and how does it relate to managed tables in Databricks?\n",
    "The Hive Metastore is a central repository that stores metadata for Apache Hive tables, including their schema (column names and data types) and location in Hadoop Distributed File System (HDFS) or other storage systems. It serves as a catalog or directory for Hive-managed tables, providing information about the structure and location of the data.\n",
    "\n",
    "- How does a managed table differ from an unmanaged (external) table in Databricks? (Hint: Consider what happens to the data when the table is deleted)\n",
    "Managed tables are those in which both the data and metadata are managed by Databricks.\n",
    "When you create a managed table, Databricks manages both the table's schema and the physical location of the data files associated with the table.\n",
    "Unmanaged tables, on the other hand, are those in which only the metadata is managed by Databricks, while the data files remain external to Databricks.\n",
    "When you create an unmanaged table, you provide a location (e.g., a directory in a storage system like HDFS, AWS S3, or Azure Blob Storage) where the data files are stored.\n",
    "\n",
    "- How can you define a schema for a managed table?\n",
    "\n",
    "In Databricks, you can define a schema for a managed table using the CREATE TABLE statement or by inferring the schema from an existing DataFrame. \n",
    "\n",
    "## Spark\n",
    "`df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location)`\n",
    "- What does the option(\"inferSchema\", \"true\") do? \n",
    " \"inferSchema\", \"true\" is used when reading data from an external data source (such as a CSV file) to instruct Spark to automatically infer the schema of the data based on its structure.\n",
    "\n",
    "- What does the option(\"header\", \"true\") do?\n",
    "\"header\", \"true\" is used when reading data from an external data source (such as a CSV file) to specify that the first row of the data contains the header or column names. \n",
    "\n",
    "- How can you write data to a managed table?\n",
    "To write data to a managed table in Databricks, we can use the DataFrameWriter API provided by Apache Spark. \n",
    "\n",
    "- How can you read data from a managed table into a DataFrame?\n",
    "To read data from a managed table into a DataFrame in Databricks, we can use the DataFrameReader API provided by Apache Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30083149-8fbc-4a35-a134-dc14ea381a90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ETL `bookings.csv` file\n",
    "\n",
    "- **Extract**: Load data from CSV file into a DF\n",
    "- **Transformation**: no transformation needed as we want to load data as it\n",
    "- **Load**: Save the DF into a managed table (or Hive table); \n",
    "\n",
    "# Managed Table\n",
    "This is an important interview topic. Some people may refer to managed tables as Hive tables.\n",
    "\n",
    "https://docs.databricks.com/data-governance/unity-catalog/create-tables.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0762a11d-a040-4b67-ab15-374eef15ed38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- bookid: integer (nullable = true)\n |-- facid: integer (nullable = true)\n |-- memid: integer (nullable = true)\n |-- starttime: timestamp (nullable = true)\n |-- slots: integer (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2434236142515292>:30\u001B[0m\n",
       "\u001B[1;32m     27\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDROP TABLE IF EXISTS bookings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Write data from DataFrame into managed table\u001B[39;00m\n",
       "\u001B[0;32m---> 30\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbookings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot create table ('`spark_catalog`.`default`.`bookings`'). The associated location ('dbfs:/user/hive/warehouse/bookings') is not empty and also not a Delta table."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2434236142515292>:30\u001B[0m\n\u001B[1;32m     27\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDROP TABLE IF EXISTS bookings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Write data from DataFrame into managed table\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbookings\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot create table ('`spark_catalog`.`default`.`bookings`'). The associated location ('dbfs:/user/hive/warehouse/bookings') is not empty and also not a Delta table.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot create table ('`spark_catalog`.`default`.`bookings`'). The associated location ('dbfs:/user/hive/warehouse/bookings') is not empty and also not a Delta table.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "\n",
    "file_location = \"/FileStore/tables/bookings.csv\"\n",
    "\n",
    "# What does `option(\"header\", \"true\")` and `option(\"inferSchema\", \"true\")` do?\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_location)\n",
    "\n",
    "# Why the df schema doesn't match the DDL data type? https://pgexercises.com/gettingstarted.html (hint: `option(\"inferSchema\", \"true\")`)\n",
    "df.printSchema()\n",
    "\n",
    "# Here is the solution to define schema manually\n",
    "# Define schema for the bookings table\n",
    "schema = StructType([\n",
    "    StructField(\"bookid\", IntegerType(), True),\n",
    "    StructField(\"facid\", IntegerType(), True),\n",
    "    StructField(\"memid\", IntegerType(), True),\n",
    "    StructField(\"starttime\", TimestampType(), True),\n",
    "    StructField(\"slots\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read data from CSV file into DataFrame with predefined schema\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(file_location)\n",
    "\n",
    "# No \n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS bookings\")\n",
    "\n",
    "# Write data from DataFrame into managed table\n",
    "df.write.saveAsTable(\"bookings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d925a21-98b7-48e7-8dd8-9d8863826747",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Complete ETL Jobs\n",
    "\n",
    "- Complete ETL for `facilities.csv` and `members.csv`\n",
    "- Tips\n",
    "  - The Databricks community version will terminate the cluster after a few hours of inactivity. As a result, all managed tables will be deleted. You will need to rerun this notebook to perform the ETL on all files for the other exercises.\n",
    "  - DBFS data will not be deleted when a custer become inactive/deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdad4ceb-f2f3-4fa9-8be1-260eb2f254ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a ETL job for `facilities.csv`\n",
    "# Define the file location\n",
    "file_location = \"/FileStore/tables/facilities.csv\"\n",
    "\n",
    "# Read data from CSV into DataFrame\n",
    "facilities_df = spark.read.format(\"csv\") \\\n",
    "                         .option(\"header\", \"true\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .load(file_location)\n",
    "\n",
    "# Optionally, define schema manually if needed\n",
    "# schema = StructType([\n",
    "#     StructField(\"facid\", IntegerType(), True),\n",
    "#     StructField(\"name\", StringType(), True),\n",
    "#     StructField(\"membercost\", DecimalType(10, 2), True),\n",
    "#     StructField(\"guestcost\", DecimalType(10, 2), True),\n",
    "#     StructField(\"initialoutlay\", DecimalType(10, 2), True),\n",
    "#     StructField(\"monthlymaintenance\", DecimalType(10, 2), True)\n",
    "# ])\n",
    "\n",
    "# Load data into a managed table\n",
    "facilities_df.write.mode(\"overwrite\").saveAsTable(\"facilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d372e005-1033-41b0-8742-5995a409d13b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a ETL job Complete ETL for `members.csv`\n",
    "# Define the file location\n",
    "file_location = \"/FileStore/tables/members.csv\"\n",
    "\n",
    "# Read data from CSV into DataFrame\n",
    "members_df = spark.read.format(\"csv\") \\\n",
    "                       .option(\"header\", \"true\") \\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .load(file_location)\n",
    "\n",
    "# Optionally, define schema manually if needed\n",
    "# schema = StructType([\n",
    "#     StructField(\"memid\", IntegerType(), True),\n",
    "#     StructField(\"surname\", StringType(), True),\n",
    "#     StructField(\"firstname\", StringType(), True),\n",
    "#     StructField(\"address\", StringType(), True),\n",
    "#     StructField(\"zipcode\", IntegerType(), True),\n",
    "#     StructField(\"telephone\", StringType(), True),\n",
    "#     StructField(\"recommendedby\", IntegerType(), True),\n",
    "#     StructField(\"joindate\", TimestampType(), True)\n",
    "# ])\n",
    "\n",
    "# Load data into a managed table\n",
    "members_df.write.mode(\"overwrite\").saveAsTable(\"members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9aa5dc-ea21-4912-963f-67678b129ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save your work to Git\n",
    "\n",
    "- Export the notebook to IPYTHON format, `notebook top menu bar -> File -> Export -> iphython`\n",
    "- Upload to your Git repository, `your_repo/spark/notebooks/`\n",
    "- Github can render ipython notebook https://github.com/josephcslater/JupyterExamples/blob/master/Calc_Review.ipynb"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3267011519595935,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "0 - ETL pgexercieses CSV files",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
