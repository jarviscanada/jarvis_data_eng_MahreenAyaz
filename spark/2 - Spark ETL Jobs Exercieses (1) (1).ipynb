{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------------------+-----+\n|bookid|facid|memid|          starttime|slots|\n+------+-----+-----+-------------------+-----+\n|     0|    3|    1|2012-07-03 11:00:00|    2|\n|     1|    4|    1|2012-07-03 08:00:00|    2|\n|     2|    6|    0|2012-07-03 18:00:00|    2|\n|     3|    7|    1|2012-07-03 19:00:00|    2|\n|     4|    8|    1|2012-07-03 10:00:00|    1|\n+------+-----+-----+-------------------+-----+\nonly showing top 5 rows\n\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|memid| surname|firstname|             address|zipcode|     telephone|recommendedby|           joindate|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\n|    0|   GUEST|    GUEST|               GUEST|      0|(000) 000-0000|         null|2012-07-01 00:00:00|\n|    1|   Smith|   Darren|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:02:05|\n|    2|   Smith|    Tracy|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:08:23|\n|    3|  Rownam|      Tim|23 Highway Way, B...|  23423|(844) 693-0723|         null|2012-07-03 09:32:15|\n|    4|Joplette|   Janice|20 Crossing Road,...|    234|(833) 942-4710|            1|2012-07-03 10:25:05|\n+-----+--------+---------+--------------------+-------+--------------+-------------+-------------------+\nonly showing top 5 rows\n\n+-----+---------------+----------+---------+-------------+------------------+\n|facid|           name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+---------------+----------+---------+-------------+------------------+\n|    0| Tennis Court 1|         5|       25|        10000|               200|\n|    1| Tennis Court 2|         5|       25|         8000|               200|\n|    2|Badminton Court|         0|     15.5|         4000|                50|\n|    3|   Table Tennis|         0|        5|          320|                10|\n|    4| Massage Room 1|        35|       80|         4000|              3000|\n+-----+---------------+----------+---------+-------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Write your solution here\n",
    "#Extract data from the managed tables.\n",
    "\n",
    "bk = spark.sql(\"select * from bookings\")\n",
    "bk.show(5)\n",
    "\n",
    "mem= spark.sql(\"SELECT* FROM members\")\n",
    "mem.show(5)\n",
    "\n",
    "fac= spark.sql(\"select * from facilities\")\n",
    "fac.show(5)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|facid|TotalSlots|\n+-----+----------+\n|    5|     122.0|\n|    3|     422.0|\n|    7|     426.0|\n|    8|     471.0|\n|    6|     540.0|\n|    2|     570.0|\n|    1|     588.0|\n|    0|     591.0|\n|    4|     648.0|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Transform data as requested into dataframe.\n",
    "\n",
    "#Produce a list of the total number of slots booked per facility in the month of September 2012. Produce an output table consisting of facility id and slots, sorted by the number of slots.\n",
    "\n",
    "from pyspark.sql.functions import col,sum\n",
    "\n",
    "Slots= bk.filter((col(\"starttime\") >= \"2012-09-01\") & (col(\"starttime\") < \"2012-10-01\"))\n",
    "#Slots.show()\n",
    "\n",
    "totalslots= Slots.groupBy(bk.facid).agg(sum('slots').alias('TotalSlots')).orderBy('TotalSlots').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2ddc6b-fabe-4ab6-a44f-68b49233be14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|          Name|      facility|\n+--------------+--------------+\n|    Anne Baker|Tennis Court 1|\n|    Anne Baker|Tennis Court 2|\n|  Burton Tracy|Tennis Court 1|\n|  Burton Tracy|Tennis Court 2|\n|  Charles Owen|Tennis Court 1|\n|  Charles Owen|Tennis Court 2|\n|  Darren Smith|Tennis Court 2|\n| David Farrell|Tennis Court 1|\n| David Farrell|Tennis Court 2|\n|   David Jones|Tennis Court 1|\n|   David Jones|Tennis Court 2|\n|  David Pinker|Tennis Court 1|\n| Douglas Jones|Tennis Court 1|\n| Erica Crumpet|Tennis Court 1|\n|Florence Bader|Tennis Court 1|\n|Florence Bader|Tennis Court 2|\n|   GUEST GUEST|Tennis Court 1|\n|   GUEST GUEST|Tennis Court 2|\n|Gerald Butters|Tennis Court 1|\n|Gerald Butters|Tennis Court 2|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Transform data as per request\n",
    "\n",
    "#How can you produce a list of all members who have used a tennis court? Include in your output the name of the court, and the name of the member formatted as a single column. Ensure no duplicate data, and order by the member name followed by the facility name.\n",
    "\n",
    "from pyspark.sql.functions import concat,concat_ws,asc,desc\n",
    "\n",
    "li= ['Tennis Court 1','Tennis Court 2']\n",
    "\n",
    "data = mem.join(bk, mem.memid==bk.memid, 'inner')\\\n",
    ".join(fac, bk.facid==fac.facid, 'inner')\n",
    "\n",
    "data1= data.select(concat_ws(' ',mem.firstname, mem.surname).alias('Name'),fac.name.alias('facility')).filter((fac.name.isin(li)))\n",
    "\n",
    "tennis= data1.distinct().orderBy('Name','facility')\n",
    "tennis.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set up your Alpha Vantage API key and base URL\n",
    "api_key = \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "base_url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "# List of companies (symbols) you want to fetch data for\n",
    "#companies = [\"GOOGL\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"StockPriceAnalysis\").getOrCreate()\n",
    "\n",
    "# Loop through each company and fetch data\n",
    "\n",
    "#GOOGL\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"GOOGL\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response1 = requests.get(base_url, params=params, headers=headers)\n",
    "data1 = response1.json()\n",
    "\n",
    "#AAPL\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"AAPL\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response2 = requests.get(base_url, params=params, headers=headers)\n",
    "data2 = response2.json()\n",
    "\n",
    "#MSFT\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"MSFT\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response3 = requests.get(base_url, params=params, headers=headers)\n",
    "data3 = response3.json()\n",
    "\n",
    "#TSLA\n",
    "params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": \"TSLA\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"4a3c8ab6f6mshf33c91629f0ecd6p194a71jsn0810a5a1a065\"\n",
    "    }\n",
    "    \n",
    "response4 = requests.get(base_url, params=params, headers=headers)\n",
    "data4 = response4.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11aadc4-8efa-41db-88b7-5cf8f20d8323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+-------+---------+\n|year|week| close|company|max_close|\n+----+----+------+-------+---------+\n|2023|  52|193.15|   AAPL|   193.15|\n|2023|  52|193.58|   AAPL|   193.58|\n|2023|  52|141.52|  GOOGL|   141.52|\n|2023|  52|140.37|  GOOGL|   140.37|\n|2023|  52|139.69|  GOOGL|   139.69|\n|2023|  52|140.23|  GOOGL|   140.23|\n|2023|  52|193.05|   AAPL|   193.05|\n|2023|  52|192.53|   AAPL|   192.53|\n|2023|  52|375.28|   MSFT|   375.28|\n|2023|  52|376.04|   MSFT|   376.04|\n|2023|  52|374.07|   MSFT|   374.07|\n|2023|  52|374.66|   MSFT|   374.66|\n|2023|  52|256.61|   TSLA|   256.61|\n|2023|  52|253.18|   TSLA|   253.18|\n|2023|  52|248.48|   TSLA|   248.48|\n|2023|  52|261.44|   TSLA|   261.44|\n|2023|  51|136.65|  GOOGL|   136.65|\n|2023|  51|138.34|  GOOGL|   138.34|\n|2023|  51|141.49|  GOOGL|   141.49|\n|2023|  51|140.42|  GOOGL|   140.42|\n+----+----+------+-------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # Convert JSON data to a PySpark DataFrame\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, year, weekofyear, max,lit\n",
    "# Create a list to store DataFrames for each company\n",
    "dataframes = []\n",
    "#companies = [\"GOOGL\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "#GOOGL\n",
    "\n",
    "stock_data1 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]), \n",
    "                int(values[\"5. volume\"])) for date, values in data1[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df1 = spark.createDataFrame(stock_data1, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df1 = stock_df1.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df1 = stock_df1.withColumn(\"company\", lit(\"GOOGL\"))\n",
    "dataframes.append(stock_df1)\n",
    "#stock_df1.show()  \n",
    "\n",
    "#AAPL\n",
    "stock_data2 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]),\n",
    "                 int(values[\"5. volume\"]))for date, values in data2[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df2 = spark.createDataFrame(stock_data2, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df2 = stock_df2.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df2 = stock_df2.withColumn(\"company\", lit(\"AAPL\"))\n",
    "dataframes.append(stock_df2)\n",
    "#stock_df2.show() \n",
    "\n",
    "#MSFT\n",
    "stock_data3 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]),\n",
    "                 int(values[\"5. volume\"]))for date, values in data3[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df3 = spark.createDataFrame(stock_data3, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df3 = stock_df3.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df3 = stock_df3.withColumn(\"company\", lit(\"MSFT\"))\n",
    "dataframes.append(stock_df3)\n",
    "#stock_df3.show() \n",
    "\n",
    "\n",
    "#TSLA\n",
    "stock_data4 = [(date, float(values[\"1. open\"]), float(values[\"2. high\"]), float(values[\"3. low\"]), float(values[\"4. close\"]),\n",
    "                 int(values[\"5. volume\"]))for date, values in data4[\"Time Series (Daily)\"].items()]\n",
    "columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "stock_df4 = spark.createDataFrame(stock_data4, columns)\n",
    "    \n",
    "# Convert date column to a proper date type\n",
    "stock_df4 = stock_df4.withColumn(\"date\", to_date(col(\"date\")))\n",
    "stock_df4 = stock_df4.withColumn(\"company\", lit(\"TSLA\"))\n",
    "dataframes.append(stock_df4)\n",
    "#stock_df4.show() \n",
    "\n",
    "\n",
    "\n",
    "all_data_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    all_data_df = all_data_df.union(df)\n",
    "\n",
    "all_data_df = all_data_df.withColumn(\"year\", year(col(\"date\")))\n",
    "all_data_df = all_data_df.withColumn(\"week\", weekofyear(col(\"date\")))\n",
    "\n",
    "\n",
    "# Group by year and week and find the maximum closing price\n",
    "weekly_max_closing = all_data_df.groupBy(\"year\", \"week\", \"close\",\"company\").agg(max(\"close\").alias(\"max_close\")).orderBy(\"week\", ascending= False)\n",
    "\n",
    "# Show the result\n",
    "weekly_max_closing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76fd671-daa8-4741-a3c0-ec4c89f43607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n|year|week| close|max_close|\n+----+----+------+---------+\n|2023|  52|193.05|   193.05|\n|2023|  52|192.53|   192.53|\n|2023|  52|193.58|   193.58|\n|2023|  52|193.15|   193.15|\n|2023|  51|195.89|   195.89|\n|2023|  51|196.94|   196.94|\n|2023|  51|194.83|   194.83|\n|2023|  51| 193.6|    193.6|\n|2023|  51|194.68|   194.68|\n|2023|  50|198.11|   198.11|\n|2023|  50|197.96|   197.96|\n|2023|  50|197.57|   197.57|\n|2023|  50|193.18|   193.18|\n|2023|  50|194.71|   194.71|\n|2023|  49|195.71|   195.71|\n|2023|  49|192.32|   192.32|\n|2023|  49|194.27|   194.27|\n|2023|  49|193.42|   193.42|\n|2023|  49|189.43|   189.43|\n|2023|  48|189.37|   189.37|\n+----+----+------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "weekly_max_closing.write.partitionBy(\"company\").mode('overwrite').parquet(\"/FileStore/tables/weekly_max_closing.parquet\")\n",
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet/company=AAPL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab0da29-0010-41c5-91cf-90c8451e8044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n|year|week| close|max_close|\n+----+----+------+---------+\n|2023|  52|141.52|   141.52|\n|2023|  52|140.37|   140.37|\n|2023|  52|139.69|   139.69|\n|2023|  52|140.23|   140.23|\n|2023|  51|140.42|   140.42|\n|2023|  51| 135.8|    135.8|\n|2023|  51|136.65|   136.65|\n|2023|  51|138.34|   138.34|\n|2023|  51|141.49|   141.49|\n|2023|  50| 132.6|    132.6|\n|2023|  50|131.94|   131.94|\n|2023|  50|132.57|   132.57|\n|2023|  50|133.29|   133.29|\n|2023|  50|132.52|   132.52|\n|2023|  49|136.93|   136.93|\n|2023|  49|134.99|   134.99|\n|2023|  49|130.02|   130.02|\n|2023|  49|130.99|   130.99|\n|2023|  49|129.27|   129.27|\n|2023|  48|131.86|   131.86|\n+----+----+------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet/company=GOOGL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca82f859-f5e3-4b76-9c28-6d22b320d999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+---------+\n|year|week| close|max_close|\n+----+----+------+---------+\n|2023|  52|256.61|   256.61|\n|2023|  52|253.18|   253.18|\n|2023|  52|248.48|   248.48|\n|2023|  52|261.44|   261.44|\n|2023|  51|252.54|   252.54|\n|2023|  51| 254.5|    254.5|\n|2023|  51|257.22|   257.22|\n|2023|  51|247.14|   247.14|\n|2023|  51|252.08|   252.08|\n|2023|  50|239.29|   239.29|\n|2023|  50|251.05|   251.05|\n|2023|  50| 253.5|    253.5|\n|2023|  50|239.74|   239.74|\n|2023|  50|237.01|   237.01|\n|2023|  49|238.72|   238.72|\n|2023|  49|243.84|   243.84|\n|2023|  49|239.37|   239.37|\n|2023|  49|235.58|   235.58|\n|2023|  49|242.64|   242.64|\n|2023|  48|244.14|   244.14|\n+----+----+------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.read.parquet(\"/FileStore/tables/weekly_max_closing.parquet/company=TSLA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- upi: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- userstamp: string (nullable = true)\n |-- crc64: string (nullable = true)\n |-- len: integer (nullable = true)\n |-- seq_short: string (nullable = true)\n |-- seq_long: string (nullable = true)\n |-- md5: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Extract RNA data from a public PostgreSQL database. only 100 rows \n",
    "\n",
    "rna_100_records = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .load()\n",
    "  .limit(100)\n",
    ")\n",
    "\n",
    "rna_100_records.count()\n",
    "rna_100_records.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741f54b2-8829-4f8d-8799-e8d3ed808854",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#fetching all rows \n",
    "rna_records = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .load()\n",
    ")\n",
    "\n",
    "rna_records.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a729818a-9e60-49be-9448-ea1cbdd9208a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the DF in to a managed table called, rna_100_records\n",
    "\n",
    "#drop table if exist \n",
    "\n",
    "#spark.sql(\"DROP TABLE IF EXISTS rna_100_record\")\n",
    "rna_100_records.write.saveAsTable(\"rna_100_records_1\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
